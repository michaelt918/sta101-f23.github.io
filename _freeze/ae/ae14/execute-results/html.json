{
  "hash": "575922336e253774ee3eea93a8aeab05",
  "result": {
    "markdown": "---\ntitle: \"Likelihoods\"\nsubtitle: \"STA 101\"\nformat: html\n---\n\n\n## Bulletin\n\n- Lab 05 due Thursday\n- [Final project](https://sta101-fa22.netlify.app/projects/final-project/) released\n\n## Today\n\nBy the end of today you will...\n\n- be able to define a likelihood function\n- understand the intuition behind likelihood-based inference\n\n## Getting started\n\nDownload this application exercise by pasting the code below into your **console**\n\n```\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae14.qmd\",\ndestfile = \"ae14.qmd\")\n```\n\n## Load packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ncoin_flips = read_csv(\n  \"https://sta101-fa22.netlify.app/static/appex/data/coin_flips_LE.csv\"\n  )\n```\n:::\n\n\n## The likelihood function\n\n$$\nL(\\theta | x) = f(x | \\theta)\n$$\n\nThe likelihood of parameter(s) $\\theta$ given the data $x$ is equivalent to the density of sample $x$ given the parameter(s) $\\theta$.\n\n- If $x$ is a discrete random variable, the likelihood function is the probability of the data given $\\theta$.\n\n### Example 1\n\n$$\nX \\sim \\text{Binomial}(k, p)\n$$\n\nwhere $k$ is the number of trials and $p$ is the probability of a success. The parameters of the distribution are $k$ and $p$.\n\n:::{.callout-note}\nRecall: you can find the formula for the binomial density function in the documentation. See `?dbinom`\n:::\n\n\n#### Exercise 1\n\nImagine we flip a coin 10 times and observe 7 heads and 3 tails. All together we have:\n\n- $k$: __\n- $x$: __\n\nThere is some true $p$ for this coin. What is your best guess (or estimate), $\\hat{p}$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_success = coin_flips %>%\n  summarize(total = sum(result)) %>%\n  pull(total)\n\nlikelihood = function(p) {\n  return(dbinom(x = num_success, size = 10, prob = p))\n}\n\nggplot() +\nxlim(0, 1) +\n  geom_function(fun = likelihood) +\n  theme_bw() + \n  labs(x = \"p\", y = \"Likelihood\",\n       title = \"Likelihood of 7/10 coin flips landing heads as a function of p\") \n```\n\n::: {.cell-output-display}\n![](ae14_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nIn the example above, $p$ is the parameter we are interested in, so we would write $L(p | x)$. If we maximize $L(p | x)$, we obtain the **maximum likelihood estimate** $\\hat{p}$. The **maximum likelihood estimate** is the value of the parameter that maximizes the likelihood function.\n\n\n## Example 2\n\nLikelihood-based inference works with continuous random variables as well.\n\n$$\nX \\sim N(\\mu, \\sigma)\n$$\nwhere $\\mu$ is the mean (location) and $\\sigma$ is the standard deviation (scale). $\\mu$ and $\\sigma$ are the parameters of the distribution. \n\n#### Exercise 2\n\nImagine resting heart rates in this class are normally distributed with unknown mean $\\mu$ and a standard deviation of 5 beats per minute. You randomly sample 1 person in the class and find their heart rate is 72 beats per minute. What is your best guess for the mean, $\\hat{\\mu}$?\n\n[answer here]\n\nYou randomly sample two additional people and find their resting heart rate is 65 and 75 beats per minute. What is your new best guess $\\hat{\\mu}$ given these three data points?\n\n[answer here]\n\nUse `geom_vline()` to add your guesses to the plots below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlikelihoodNormal1 = function(mu) {\n  return(dnorm(x = 72 , mean = mu, sd = 5))\n}\n\nlikelihoodNormal2 = function(mu) {\n  return(dnorm(x = 65 , mean = mu, sd = 5))\n}\n\nlikelihoodNormal3 = function(mu) {\n  return(dnorm(x = 75 , mean = mu, sd = 5))\n}\n\n\n\nggplot() +\nxlim(50, 90) +\n  geom_function(fun = likelihoodNormal1) +\n  theme_bw() +\n  labs(x = \"Mu\",\n       y = \"Likelihood\",\n       title = \"Likelihood of mu given 1 data point\")\n```\n\n::: {.cell-output-display}\n![](ae14_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nIt's a similar story for each of the other two data points. If we want to find the likelihood of some parameter under three independent observations, we can multiply our likelihoods together.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncombinedLikelihood = function(mu) {\n  return(likelihoodNormal1(mu) * \n           likelihoodNormal2(mu) * \n           likelihoodNormal3(mu))\n}\n\nggplot() +\nxlim(50, 90) +\n  geom_function(fun = combinedLikelihood) +\n  theme_bw() +\n  labs(x = \"Mu\",\n       y = \"Likelihood\",\n       title = \"Likelihood of mu given 3 data points\")\n```\n\n::: {.cell-output-display}\n![](ae14_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWhat do you notice about the shape of the function as the number of data points increases?\n\n:::{.callout-note}\nAs the number of data points increases, the product of several likelihood functions gets very small very quickly. For this reason we often work with the log-likelihood. It is a fact that if $x < y$ then $\\log(x) < \\log(y)$. This property is known as *monotonicity*. Because of this, the maximum of the log-likelihood will be the same as the maximum of the likelihood function. \n:::\n\n### Example 3\n\nIn all of the examples above, the findings were obvious upon inspection. How do you compute the likelihood in, e.g. a regression setting? Remember that $\\text{AIC} = 2k - 2 \\log \\text{likelihood}$.\n\nA simple model:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n$$\n\nA common assumption:\n\n$$\n\\epsilon \\sim N(0, \\sigma)\n$$\n\nFrom this assumption:\n\n$$\ny \\sim N(\\beta_0 + \\beta_1x_1, \\sigma)\n$$\n\n#### Exercise 3\n\nAssume $\\sigma = 2$ is known and $\\beta_0 = 1$. You observe the data point, $(x,y) = (1, 6.5)$. What is your best estimate $\\hat{\\beta_1}$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregLikelihood = function(beta1) {\n  return(dnorm(x = 6.5, mean = (1 + (beta1*1)), sd = 2))\n}\n\nggplot() +\nxlim(-1, 12) +\n  geom_function(fun = regLikelihood) +\n  theme_bw() +\n  labs(x = \"beta1\",\n       y = \"Likelihood\",\n       title = \"Likelihood of beta1 given 1 data points\")\n```\n\n::: {.cell-output-display}\n![](ae14_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nIn the code above, why do we write `x = 6.5` when $6.5$ is the observed value of $y$?\n\n[answer here]\n\nYou observe two more data points, $(2, 10)$ and $(5,26)$. What is your best guess of $\\hat{\\beta_1}$ based on the three points you observed?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregLikelihood2 = function(beta1) {\n  return(dnorm(x = 10, mean = (1 + (beta1*2)), sd = 2))\n}\n\nregLikelihood3 = function(beta1) {\n  return(dnorm(x = 26, mean = (1 + (beta1*5)), sd = 2))\n}\n\ncombinedRegLikelihood = function(beta1){ \n  return(log(regLikelihood(beta1)) + log(regLikelihood2(beta1)) +\n           log(regLikelihood3(beta1)))\n  }\n\n\nggplot() +\nxlim(-2, 12) +\n  geom_function(fun = combinedRegLikelihood) +\n  theme_bw() +\n  labs(x = \"beta1\",\n       y = \"log-likelihood\",\n       title = \"Likelihood of beta1 given 3 data points\")\n```\n\n::: {.cell-output-display}\n![](ae14_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "ae14_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}