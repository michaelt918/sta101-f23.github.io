{
  "hash": "1f8e82458401b62d0e8cdf731dc50cd0",
  "result": {
    "markdown": "---\ntitle: \"Model Selection\"\nsubtitle: \"STA 101\"\nformat: html\n---\n\n\n## Bulletin\n\n- Please take the first five minutes of class to fill out [this survey](https://duke.qualtrics.com/jfe/form/SV_0oIT7pdlmr1FKJM)\n\n- My Wednesday office hours are now zoom only for the rest of the semester. Monday will remain hybrid.\n\n## Today\n\nBy the end of today you will...\n\n- select between linear models with different numbers of predictors\n\n## Getting started\n\nDownload this application exercise by pasting the code below into your **console** (bottom left of screen)\n\n```\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae9.qmd\",\ndestfile = \"ae9.qmd\")\n```\n\n## Load packages and data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n:::\n\n\n## Notes\n\n### The problem with $R^2$\n\n$R^2$ tell us the proportion of variability in the data our model explains. If we add predictors to our model, we will *always* improve $R^2$ (regardless of whether the predictor is good or not). \n\nTo see this...\n\n- offline example\n\n- take away: a line can go through any two points, a plane can go through any three points, etc. In general an $n$ dimensional object can go through $n$ points.\n\nFor this reason, $R^2$ is not a good way to select between two models that have a different number of predictors. Instead, we prefer to use Akaike Information Criterion (AIC). \n\n### AIC \n\n$$\n\\text{AIC} = 2k - 2 \\log (\\text{likelihood})\n$$\n\nwhere $k$ is the number of estimated parameters ($\\beta$s) in the model. Notice this will be 1 + the number of predictors. and $\\hat{L}$ is \"likelihood\" of the data given the fitted model. \n\nWe'll return to the idea of a likelihood later in the semester. For now, it suffices to know that the likelihood is a measure of how well a given model fits the data. Specifically, higher likelihoods imply better fits. Since the AIC score has a negative in front of the log likelihood, lower scores are better fits. However, $k$ penalizes adding new predictors to the model.\n\nTake-away: lower AIC is better fit.\n\nYou can find AIC using `glance(fitted-model)`. (Assuming you named your fitted model `fitted-model`)\n\n## Building a model\n\nScenario: you have an outcome $y$ you want to predict. You have several variables you've measured that you *could* use as predictors in your linear model. Each predictor is expensive to collect future measurements of. You want your model to only include the most useful predictors. \n\n### Backward elimination \n\nBackward elimination starts with the full model (the model that includes all potential predictor variables). Variables are eliminated one-at-a-time from the model until we cannot improve the model any further.[^1]\n\nProcedure:\n\n1. Start with a model that has all predictors we consider and compute the AIC.\n2. Next fit every possible model with 1 less predictor.\n3. Compare AIC scores to select the best model with 1 less predictor.\n4. Repeat steps 2 and 3 until you score the model with no predictors.\n5. Compare AIC among all tested models to select the best model.\n\n\n### Forward selection\n\nForward selection is the reverse of the backward elimination technique. Instead, of eliminating variables one-at-a-time, we add variables one-at-a-time until we cannot find any variables that improve the model any further.\n\n[^1]: this definition is from [Introduction to Modern Statistics](https://openintro-ims.netlify.app/model-mlr.html?q=selection#stepwise-selection).\n\nProcedure:\n\n1. Start with a model that has no predictors.\n2. Next fit every possible model with 1 additional predictor and score each model.\n3. Compare AIC scores to select the best model with 1 additional predictor.\n4. Repeat steps 2 and 3 until you score the model with all available predictors.\n5. Compare AIC among all tested models to select the best model.\n\n\n## Example\n\n### Exercise \n\n- Will forward selection and backward elimination always yield the same model? Type your answer below before running any code.\n\n- Next, see if you are right in using the data set below.\n\n**Solution below**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_df = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/test_df.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 20 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): y, x1, x2, x3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n\nIn the following two examples, we will use stepwise selection to build a **main effects** model.\n\nPerform 1 step of forward selection. What variable will be **in** the final *forward selection* model?\n\n- Answer: $x_1$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x1, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 93.08637\n```\n:::\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x2, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 131.8917\n```\n:::\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x3, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 152.9043\n```\n:::\n:::\n\n\nNext, perform 1 step of backward elimination. Which variable will **not** be in the final *backward elimination* model?\n\n- Answer: $x_1$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x2 + x3, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 35.25949\n```\n:::\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x1 + x3, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 94.2536\n```\n:::\n\n```{.r .cell-code}\nlinear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(y ~ x1 + x2, data = test_df) %>%\n  glance() %>%\n  pull(AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 87.1686\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}