---
title: Regression with a single predictor
subtitle: Lecture 6
format: revealjs
auto-stretch: false
---

# Warm up

## Announcements

-   Lab 3 is due Thu, Sep 21 at 5 pm on Gradescope
-   No class on Wednesday, catch up on readings, interactive tutorials, labs!

# Regression with a single predictor

## Data and packages

We'll work with data on Apple and Microsoft stock prices and use the **tidyverse** and **tidymodels** packages.

```{r}
#| message: false

library(tidyverse)
library(tidymodels)

stocks <- read_csv("data/stocks.csv")
```

## Simple regression model and notation {.smaller}

$$
y = \beta_0 + \beta_1 x + \epsilon
$$

::: incremental
-   $y$: the **outcome** variable. Also called the "response" or "dependent variable". In prediction problems, this is what we are interested in predicting.

-   $x$: the **predictor**. Also commonly referred to as "regressor", "independent variable", "covariate", "feature", "the data".

-   $\beta_0$, $\beta_1$ are called "constants" or **coefficients**. They are fixed numbers. These are **population parameters**. $\beta_0$ has another special name, "the intercept".

-   $\epsilon$: the **error**. This quantity represents observational error, i.e. the difference between our observation and the true population-level expected value: $\beta_0 + \beta_1 x$.
:::

. . .

Effectively this model says our data $y$ is linearly related to $x$ but is not perfectly observed due to some error.

## Stock prices of Microsoft and Apple

Let's examine January 2020 open prices of Microsoft and Apple stocks to illustrate some ideas.

```{r}
#| label: stocks-jan2020
#| output-location: slide

stocks_jan2020 <- stocks |>
  filter(month(date) == 1 & year(date) == 2020)

ggplot(stocks_jan2020, aes(x = MSFT.Open, y = AAPL.Open)) +
  geom_point() + 
  labs(
    x = "MSFT Open", 
    y = "AAPL Open", 
    title = "Open prices of MSFT and AAPL",
    subtitle = "January 2020"
  )
```

## Fitting "some" model

Before we get to fitting the best model, let's fit "some" model, say with slope = -5 and intercept = 0.5.

```{r}
#| label: stocks-jan2020-some-model
#| code-line-numbers: "|3"
#| output-location: slide

ggplot(stocks_jan2020, aes(x = MSFT.Open, y = AAPL.Open)) +
  geom_point() + 
  geom_abline(slope = 0.5, intercept = -5) +
  labs(
    x = "MSFT Open", 
    y = "AAPL Open", 
    title = "Open prices of MSFT and AAPL",
    subtitle = "January 2020"
  )
```

## Fitting "some" model

$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 ~ x \\
\hat{y} = -5 + 0.5 ~ x
$$

-   $\hat{y}$ is the **expected outcome**
-   $\hat{\beta}$ is the **estimated** or **fitted** coefficient
-   There is no error term here because we do not predict error

## Populations vs. samples

::: columns
::: {.column width="50%"}
Population:

$$
y = \beta_0 + \beta_1 ~ x
$$
:::

::: {.column width="50%"}
Samples: $$
\hat{y} = \hat{\beta_0} +  \hat{\beta_1} ~ x
$$
:::
:::

::: incremental
-   The central idea is that if we measure every $x$ and every $y$ in existence, ("the entire population") there is some true "best" $\beta_0$ and $\beta_1$ that describe the relationship between $x$ and $y$
-   Since we only have a **sample** of the data, we estimate $\beta_0$ and $\beta_1$
-   We call our estimates $\hat{\beta_0}$, $\hat{\beta_1}$ "beta hat". We never have all the data, thus we never can really know what the true $\beta$s are
:::

## Residuals

-   For any linear equation we write down, there will be some difference between the predicted outcome of our linear model ($\hat{y}$) and what we observe ($y$)... (But of course! Otherwise everything would fall on a perfect straight line!)

This difference between what we observe and what we predict $y - \hat{y}$ is known as a residual, $e$.

More concisely,

$$
e = y - \hat{y}
$$

## A residual, visualized

Residuals are dependent on the line we draw. Visually, here is a model of the data, $y = -5 + 0.5 ~ x$ and one of the residuals is outlined in red.

```{r}
#| echo: false
#| fig-width: 8
#| fig-asp: 0.618
#| fig-align: center

# day11 = c(164.35, 78.4) # data point

ggplot(stocks_jan2020, aes(x = MSFT.Open, y = AAPL.Open)) +
  geom_point() + 
  geom_abline(slope = 0.5, intercept = -5) +
  geom_segment(x = 164.35, xend = 164.35, y  = 78.4, yend = 77.175, color = "red") +
  labs(
    x = "MSFT Open", 
    y = "AAPL Open", 
    title = "Open prices of MSFT and AAPL January 2020"
  )
```

## All residuals, visualized

There is, in fact, a residual associated with every single point in the plot.

```{r}
#| label: all-residuals
#| echo: false

predictAAPL <- function(x) {
  return(-5 + (0.5*x))
}

xPoints <- stocks_jan2020$MSFT.Open
yPoints <- stocks_jan2020$AAPL.Open
yHat <- predictAAPL(xPoints)

stocks_jan2020 |>
  ggplot(aes(x = MSFT.Open, y = AAPL.Open)) +
  geom_point() + 
  labs(x = "MSFT Open", y = "AAPL Open", title = "Open prices of MSFT and AAPL January 2020") +
  geom_abline(slope = 0.5, intercept = -5) +
  geom_segment(x = xPoints, xend = xPoints, y  = yPoints, yend = yHat, color = "red")
```

## Minimize residuals

We often wish to find a line that fits the data "really well", but what does this mean? Well, we want small residuals! So we pick an **objective function**. That is, a function we wish to minimize or maximize.

## Application exercise

Go to Posit Cloud and start the project titled **ae-06-Stocks**.
